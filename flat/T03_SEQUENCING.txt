# === FLAT FILE: T03_SEQUENCING.txt ===
# Version: 2025.04.20
# Created by: Mark + ChatGPT
# Included Modules in (THIS) T02_INITIAL_PROCESSING:
#   - FILE: core/control/sequence_controller.py
#   - FILE: core/boot/pathfix.py
#   - FILE: core/gui/prompt_simulator_window.py
#   - FILE: core/control/simulated_dispatcher.py
#   - FILE: core/utils/reflex_token_parser.py ===

### Structure Changelog

| Date           |                                              Change Summary                                            |
|----------------|--------------------------------------------------------------------------------------------------------|
| **2025-04-20** | Structure Changelog resumes HERE in T03_SEQUENCING.txt".                                               |
| **2025-04-20** | Sequencer utility GUI launched from menu item "Launch Prompt Cycle Test".                              |
| **2025-04-22** | Files core/control/sequence_controller.py and core/boot/pathfix.py moved out of T02_INITIAL_PROCESSING |
| **2025-04-22** | Files core/control/sequence_controller.py and core/boot/pathfix.py moved into T03_SEQUENCING (THIS)    |
| **2025-04-23** | All issues with the sequencer have been fixed. It can now pass messages to the utility windows.        |
| **2025-04-23** | File core/utils/reflex_token_parser.py was created.                                                    |
|----------------|--------------------------------------------------------------------------------------------------------|

---



# === FILE NAME: core/control/sequence_controller.py ===

"""
Aurora – Reflexive AI Control Framework
---------------------------------------

Module: core/control/sequence_controller.py
Authors: ChatGPT and Mark
Created: 2025-04-16
Location: Evans, Colorado
Project: Aurora

Main orchestrator for executing sequence-driven reflex logic. This module reads
step instructions from the database, triggers reflex execution via dispatcher,
tracks jump/repeat state, and evaluates completion or interrupts.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T02_INITIAL_PROCESSING.txt
    - This file participates in the T02-B04_SEQ_CONT branch of development.
    - All session behaviors are tracked and logged through flat file modules.

---

Sequence Controller

Executes step-based sequences stored in the database. Supports step jumping,
bounded loops, reflex triggering, and future interrupt integration.
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from data.db_interface import load_sequence_steps # ************************************ Chat with aurora.db to use the "load_sequence_steps" function

class SequenceController:
    def __init__(self, sequence_id: int, simulated: bool = False):
        self.simulated = simulated
        self.sequence_id = sequence_id
        self.steps = load_sequence_steps(sequence_id) # ******************************** Chat with aurora.db to retrieve the sequence id
        self.index = 0
        self.index_store = 0
        self.loop_count = 0
        self.resume = 0
        self.history = []
        if simulated:
            from core.control import simulated_dispatcher as dispatcher # ************* If we're simulated, grab the "simulated_dispatcher" routine
        else:
            from core.control import reflex_dispatcher as dispatcher # **************** If we're not simulated, grab the "reflex_dispatcher" routine

        self.dispatch_step = dispatcher.dispatch_step # ******************************* CALL DISPATCH ROUTINE, RETURNS STEPS **********************

    def run(self): # ****************************************************************** Imaginatively named function "run" - run a loop

        print(f"[SequenceController] Starting sequence {self.sequence_id}...") # ****** Debug print sequence startup data

        while self.index < len(self.steps):

            if self.resume:
                step = self.steps[self.index_store]
            else:
                step = self.steps[self.index]


            print(f"[SequenceController] Length of self.steps is: {len(self.steps)}")
            print(f"[SequenceController] Executing step_order {step['step_order']} (index {self.index}): {step.get('instruction')}")
            result = self.dispatch_step(step)

            if isinstance(result, str) and "step complete" in result: # ************* If the function returned "step complete" *****************
                print(f"[SequenceController] Step {self.index} Complete. Awaiting next trigger. Holding...")
                self.resume = 1
                self.index_store = self.index + 1
                continue

            elif isinstance(result, str) and "step incomplete" in result: # ************ If the function returned "step incomplete" **************
                print(f"[SequenceController] Step {self.index} incomplete due to missing response trigger. Holding...")
                self.resume = 1
                self.index_store = self.index
                continue

            else:
                print(f"[SequenceController] Step {self.index} would have runaway. Result was: {result}")
                self.resume = 0
                #break
                continue

            self.history.append({
                "index": self.index,
                "step": step,
                "result": result
            })

            # Handle per-step repeat logic
            step_repeat = step.get("repeat", 0)
            if step_repeat > 0:
                if "repeat_count" not in step:
                    step["repeat_count"] = 1
                else:
                    step["repeat_count"] += 1

                if step["repeat_count"] < step_repeat:
                    print(f"[SequenceController] Repeating step {self.index} ({step['repeat_count']}/{step_repeat})")
                    continue
                else:
                    print(f"[SequenceController] Step {self.index} reached repeat limit. Continuing.")
                    step["repeat_count"] = 0  # Optional cleanup

            # Check for jump
            if step.get('jump') not in (None, 0):
                target_step_order = step['jump']
                rpt = step.get('jmp_rpt', 0)

                if rpt == 0 or self.loop_count < rpt:
                    # Locate the step with matching step_order
                    target_index = next(
                        (i for i, s in enumerate(self.steps) if s["step_order"] == target_step_order),
                        None
                    )

                    if target_index is None:
                        print(f"[SequenceController] ERROR: jump target step_order {target_step_order} not found. Skipping jump.")
                    else:
                        loop_display = f"{self.loop_count + 1}/{rpt}" if rpt else f"{self.loop_count + 1}/(infinite)"

                        if target_index == self.index and rpt == 0:
                            print(f"[SequenceController] ERROR: Infinite self-jump detected at index {self.index}. Aborting sequence.")
                            break

                        print(f"[SequenceController] Jumping to step_order {target_step_order} (index {target_index}) — loop {loop_display}")
                        self.index = target_index
                        self.loop_count += 1
                        continue
                else:
                    print("[SequenceController] Jump limit reached. Continuing.")
                    self.loop_count = 0

        self.index += 1

        print(f"[SequenceController] Sequence {self.sequence_id} completed.")

if __name__ == "__main__":
    from data.db_interface import get_sequence_id_by_name

    seq_id = get_sequence_id_by_name("SHOW MODE")
    if seq_id is None:
        print("[SequenceController] ERROR: Sequence 'SHOW MODE' not found.")
        sys.exit(1)

    controller = SequenceController(sequence_id=seq_id)
    controller.run()



# === FILE NAME: core/boot/pathfix.py ===

"""
Aurora – Reflexive AI Control Framework
---------------------------------------

Module: core/boot/pathfix.py
Authors: ChatGPT and Mark
Created: 2025-04-17
Location: Evans, Colorado
Project: Aurora

This utility module provides a centralized method to patch the Python sys.path
during local development. It ensures consistent import resolution from any
submodule within the Aurora project when executed in non-package mode.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T02_INITIAL_PROCESSING.txt
    - Category: Bootstrap Utility
    - Used by all modules requiring guaranteed path stability across contexts.

---

Path Bootstrap Utility

Call patch_path() at the top of any deep module to ensure that the root
project path is visible to Python, enabling absolute imports of all
core.* modules and subpackages.
"""

import sys
import os

def patch_path():
    root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if root not in sys.path:
        sys.path.insert(0, root)
        print(f"[PATHFIX] Root path added: {root}")
    else:
        print("[PATHFIX] Root path already present.")



# === FILE NAME: core/gui/prompt_simulator_window.py ===

"""
Aurora – Reflexive AI Control Framework
---------------------------------------

Module: core/gui/prompt_simulator_window.py
Authors: ChatGPT and Mark
Created: 2025-04-20
Location: Evans, Colorado
Project: Aurora

This module implements the Prompt Cycle Simulation Utility. It replaces live browser
interaction with a GUI mock environment for testing step-based prompt sequences without
launching ChatGPT or consuming tokens. Prompts intended for the browser are intercepted,
and a manual response may be entered to simulate GPT output.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T03_SEQUENCING.txt
    - This file participates in the T02-B04_SEQ_CONT branch of development.
    - All session behaviors are tracked and logged through flat file modules.

---

Prompt Simulation Utility

Displays prompts that would be issued by the reflex dispatcher,
allows user to supply simulated responses, and provides UI feedback
for step flow and interaction without launching a browser.
"""

from PySide6.QtWidgets import (
    QDialog, QVBoxLayout, QHBoxLayout,
    QLabel, QListWidget, QTextEdit, QLineEdit, QPushButton
)

from PySide6.QtCore import QThread, QObject, Signal, Slot, Qt, QMetaObject

from core.control import simulated_dispatcher

class PromptSimulatorWindow(QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Aurora – Prompt Cycle Simulator")
        self.setMinimumSize(700, 500)

        self.layout = QVBoxLayout(self)

        # --- Begin Sequence Button ---
        self.run_button = QPushButton("Run Sequence") # ******************************** "Run sequence" button
        self.run_button.clicked.connect(self.begin_sequence)
        self.layout.addWidget(self.run_button)

        # --- Prompt Preview ---
        self.prompt_label = QLabel("Injected Prompt (Simulated):") # ******************* Injected Prompt (Simulated) listbox
        self.prompt_display = QTextEdit()
        self.prompt_display.setReadOnly(True)

        # --- Simulated GPT Response ---
        self.reply_label = QLabel("Enter Simulated GPT Response:") # ******************* User entry box
        self.reply_input = QLineEdit()
        self.send_button = QPushButton("Send Response") # ****************************** "Send Response" button
        self.send_button.clicked.connect(self.send_response)

        # --- Step History and Response Log ---
        self.step_log = QListWidget()
        self.step_log.setFixedHeight(120)
        self.step_log_label = QLabel("Sequence Step Log:") # ************************** Sequencer Step Log listbox

        self.reply_log = QListWidget()
        self.reply_log.setFixedHeight(120)
        self.reply_log_label = QLabel("Simulated Response Log:") # ******************** Sequencer Response Log listbox

        self.status_label = QLabel("Status: Idle") # ********************************** Sequencer status label
        self.layout.addWidget(self.status_label)

        # --- Layout Assembly ---
        self.layout.addWidget(self.prompt_label) #    Injected Prompt label
        self.layout.addWidget(self.prompt_display) #  Injected Prompt text edit box
        self.layout.addWidget(self.reply_label) #     User entry box label

        hbox = QHBoxLayout()
        hbox.addWidget(self.reply_input) #            User entry box
        hbox.addWidget(self.send_button) #            User entry box send button
        self.layout.addLayout(hbox) #                 Horizonal container

        self.layout.addWidget(self.step_log_label) #  Step Log label
        self.layout.addWidget(self.step_log) #        Step Log listbox

        self.layout.addWidget(self.reply_log_label) # Response Log box label
        self.layout.addWidget(self.reply_log) #       Respons Log listbox

        from core.control.simulated_dispatcher import inject_simulator # Call function inject_simulator
        inject_simulator(self)

    @Slot(str)
    def inject_prompt(self, prompt_text: str): # *********************** Should display incoming prompts
        self.prompt_display.setPlainText(prompt_text)
        self.step_log.addItem(f"[Injected] {prompt_text[:80]}")
        print("[PromptSimulatorWindow Injected] {prompt_text[:80]}")

    def send_response(self): # ***************************************** Should display user responses
        response = self.reply_input.text().strip()
        if response:
            self.reply_log.addItem(f"[User] {response}") # ################################ REPLY LOG ##############################
            print("[PromptSimulatorWindow Added User Response:] {response}")
            self.reply_input.clear()
            self.prompt_display.clear()
            simulated_dispatcher.response_queue.put(response) # ########################## DISPATCHER RESPONSE_QUEUE ##############

    def begin_sequence(self):
        self.status_label.setText("Status: Running...")
        try:
            seq_id = self.parent().ui.pb_sequence_arm.property("sequence_id") # ******************** Grab sequence id from the arm button, assign to "seq_id"
        except AttributeError:
            print("[PromptSimulatorWindow] Could not access sequence ID.") # ****** If oops
            return

        if seq_id is None:
            print("[PromptSimulatorWindow] No sequence ID armed.") # ************** If oops
            return

        self.thread = QThread()
        self.runner = SequenceRunner(seq_id)
        self.runner.moveToThread(self.thread)
        self.thread.started.connect(self.runner.run)
        self.runner.finished.connect(self.thread.quit)
        self.runner.finished.connect(self.runner.deleteLater)
        self.thread.finished.connect(self.thread.deleteLater)

        print(f"[PromptSimulatorWindow] Running sequence {seq_id} in background thread.")
        self.thread.start() # ********************************************************************* Launch sequencer routines

    @Slot()
    def on_sequence_complete(self):
        self.status_label.setText("Status: Sequence complete.")

class SequenceRunner(QObject):
    finished = Signal()

    def __init__(self, sequence_id):
        super().__init__()
        self.sequence_id = sequence_id

    @Slot()
    def run(self):
        import threading
        print(f"[DEBUG] Running in thread: {threading.current_thread().name}")
        from core.control.sequence_controller import SequenceController
        controller = SequenceController(sequence_id=self.sequence_id, simulated=True)# ************ Assign values to pass to sequence controller
        controller.run() # ************************************************************************ Run sequence controller
        QMetaObject.invokeMethod(
            self.parent(),  # assumes parent is PromptSimulatorWindow
            "on_sequence_complete",
            Qt.QueuedConnection
        )
        self.finished.emit()

if __name__ == "__main__":
    from PySide6.QtWidgets import QApplication
    import sys
    app = QApplication(sys.argv)
    window = PromptSimulatorWindow()
    window.show()
    simulated_dispatcher.inject_simulator(window)
    sys.exit(app.exec())



# === FILE NAME: core/control/simulated_dispatcher.py ===

"""
Aurora – Reflexive AI Control Framework
---------------------------------------

Module: core/control/simulated_dispatcher.py
Authors: ChatGPT and Mark
Created: 2025-04-20
Location: Evans, Colorado
Project: Aurora

Simulated dispatcher module that replaces browser interaction with GUI-driven
manual prompt/response exchange. Used for development, testing, and verification
of sequence behavior without launching an actual ChatGPT session.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T03_SEQUENCING.txt
    - This file participates in the T02-B04_SEQ_CONT branch of development.
    - Used in place of reflex_dispatcher during simulation mode.
---

Simulated Reflex Dispatcher

Provides prompt injection and wait emulation without launching a browser.
Prompts appear in the PromptSimulatorWindow; replies are entered manually.
"""

import time
import os
import sys
from queue import Queue

# Ensure 'data' is in sys.path for correct resolution of db_interface
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from data.db_interface import resolve_reflex_action

# This global is populated externally
simulator_instance = None
response_queue = Queue()


def inject_simulator(sim): # ************************************* Called by prompt_simulator_window
    global simulator_instance
    simulator_instance = sim
    print("[SimulatedDispatcher] Simulator connected.")

def dispatch_step(step: dict) -> str: # *************************** Define function "dispatch_step"
    """
    Handle a single sequence step in simulation mode.
    Instead of browser actions, prompt and response are handled via UI.
    """
    if simulator_instance is None:
        print("[SimulatedDispatcher] ERROR: No simulator attached.")
        return "(Simulator not ready)"

    expected_id  = step.get("expected", 0) # ********************** Get the value from the "expected" field and assign it to expected_id
    if expected_id: # ********************************************* If the value in expected_id is not empty
        try:
            expected = resolve_reflex_action(expected_id) # ******* Chat with aurora.db to return the value in the "expected_id" field and assign it to "expected"
            if expected: # **************************************** If expected is not empty
                print(f"[SimulatedDispatcher] Resolved expected {expected_id} to expected: {expected}") # **** Debug print resolved value in expected
            else:
                print(f"[SimulatedDispatcher] ERROR: expected {expected_id} returned nothing.") # ************ Debug print failure to resolve value in expected_id
        except Exception as e:
            print(f"[SimulatedDispatcher] ERROR resolving expectd {expected_id}: {e}") # ********************* Debug print error resolving value in expected_id

    # If no command, attempt reflex resolution

    reflex_id = step.get("reflex_action", 0)
    if reflex_id:
        try:
            reflex = resolve_reflex_action(reflex_id)
            if reflex:
                print(f"[SimulatedDispatcher] Resolved reflex_action {reflex_id} to reflex: {reflex}")
            else:
                print(f"[SimulatedDispatcher] ERROR: reflex_action {reflex_id} returned nothing.")
        except Exception as e:
            print(f"[SimulatedDispatcher] ERROR resolving reflex_action {reflex_id}: {e}")

    # Still nothing? Bail.
    if not expected:
        print("[SimulatedDispatcher] ERROR: Step has no value in the field named  expected. Skipping.")
        return "(Value for expected not found in step)"

    elif not reflex:
        print("[SimulatedDispatcher] ERROR: Step has no value in the field named reflex. Skipping.")
        return "(Value for expected not found in step)"

    # *************************************************************************** Check the contents of the "expected" field
    if expected.startswith("PROMPT:"):
        print(f"[Command Starts With:] {expected_id}; command: {expected}")
        # Block until simulated response is received
        print("[SimulatedDispatcher] Waiting for user response...")
        while response_queue.empty():
            time.sleep(0.1)

        reply = response_queue.get()
        print(f"[SimulatedDispatcher] Received simulated reply: {reply}")

        #  *********************************************************************** Check for trigger in the "expected" field

        expected_key_id = step.get("reflex_action", 0)# ************************** Assign the string value in the "reflex_action" field
        expected_token = None

        if expected_key_id:# ***************************************************** If the "expected_key_id" value is not empty
            try:
                expected_token = resolve_reflex_action(expected_key_id)# ********* Chat with aurora.db to see if there is a match to expected_token
            except Exception as e:
                print(f"[SimulatedDispatcher] WARNING: Could not resolve expected token {expected_key_id}: {e}")# *************** If oops

            if expected_token:# ************************************************** If the "expected_token" value is not empty
                if expected_token in reply:# ************************************* If expected_token matches the user input
                    print(f"[SimulatedDispatcher] Expected token '{expected_token}' FOUND in reply.")#*************************** Debug print token found
                    return "(Simulated) Trigger match: step complete."# ********** Back to routine report "step complete" >>>>>>>>>>>>>>>>>
                else:
                    print(f"[SimulatedDispatcher] Expected token '{expected_token}' NOT FOUND. Holding at this step.")# ********* Debug print token not found
                    return "(Simulated) Trigger missing: step incomplete."# ****** Back to routine report "step incomplete" >>>>>>>>>>>>>>>
        else:
            print("[SimulatedDispatcher] No expected token defined. Proceeding by default.")# ******** If the "expected_key_id" value is empty
            return reply# ******************************************************** Back to routine return user input

    elif expected.startswith("WAIT:"):
        seconds = int(expected[len("WAIT:"):].strip())
        print(f"[SimulatedDispatcher] Simulated wait for {seconds} seconds.")
        time.sleep(seconds)
        return f"(Simulated) Waited {seconds} seconds."

    elif expected.upper().startswith("CHECK:"):
        token = expected[6:].strip().strip("/")
        print(f"[SimulatedDispatcher] Simulated check for token: /{token}/ (not implemented)")
        return f"(Simulated) CHECK:/{token}/"

    else:
        print(f"[SimulatedDispatcher] Unhandled command: {expected}")
        return f"(Simulated) Unhandled: {expected}"

    prompt_text = expected
    from PySide6.QtCore import QMetaObject, Qt, Q_ARG

    QMetaObject.invokeMethod(
        simulator_instance,
        "inject_prompt",
        Qt.QueuedConnection,
        Q_ARG(str, prompt_text)
    )

    print(f"[SimulatedDispatcher] Prompt sent to simulator: {prompt_text[:80]}")



# === FILE NAME: core/utils/reflex_token_parser.py ===

"""
Aurora – Reflexive AI Control Framework
---------------------------------------

Module: core/utils/reflex_token_parser.py
Authors: ChatGPT and Mark
Created: 2025-04-23
Location: Evans, Colorado
Project: Aurora

This module implements a parser for reflex-style tokens that adhere to the format
/TOKEN/ or /TOKEN: ARGUMENT/ as used by Aurora's reflex protocol. It extracts any tokens
found within a GPT response and returns a structured dictionary including the final
token (which acts as a step finalizer), all tokens encountered, any accompanying arguments,
and the original response text. This functionality is essential for determining if a response
meets the expected completion criteria and for capturing metadata for downstream processing.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of the
    Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: TEMPLATES.txt
    - Conforms to Aurora’s reflex token protocol design.
"""

import re

def parse_reflex_tokens_with_args(response_text, expected=None):
    """
    Parses reflex-style tokens of the format /TOKEN/ or /TOKEN: ARGUMENT/ from response text.

    Args:
        response_text (str): The full GPT response or any string Aurora should parse.
        expected (str, optional): If provided, the parser will check if the final token matches this.

    Returns:
        dict: {
            "matched": bool,           # Whether the final token matched expected.
            "finalizer": str or None,  # The last token found.
            "all_tokens": list of str, # All /TOKEN/ strings found.
            "arguments": dict,         # Mapping of each token to its ARGUMENT (or None if not present).
            "raw": str,                # The original response text.
            "reason": str,             # Explanation if no tokens were found.
        }
    """
    pattern = r"/([A-Z_]+)(?::([^/]+))?/"
    matches = re.findall(pattern, response_text)

    if not matches:
        return {
            "matched": False,
            "finalizer": None,
            "all_tokens": [],
            "arguments": {},
            "raw": response_text,
            "reason": "No tokens found"
        }

    tokens = [f"/{token}/" for token, _ in matches]
    final_token = tokens[-1]
    matched = final_token == expected if expected else True

    return {
        "matched": matched,
        "finalizer": final_token,
        "all_tokens": tokens,
        "arguments": {f"/{token}/": arg.strip() if arg else None for token, arg in matches},
        "raw": response_text
    }



[END T03_SEQUENCING.txt]
