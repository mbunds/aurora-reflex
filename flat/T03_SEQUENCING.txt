# === FLAT FILE: T03_SEQUENCING.txt ===
# Version: 2025.04.20
# Created by: Mark + ChatGPT
# Included Modules in (THIS) T02_INITIAL_PROCESSING:
#   - FILE: core/control/sequence_controller.py
#   - FILE: core/boot/pathfix.py
#   - FILE: core/gui/prompt_simulator_window.py
#   - FILE: core/control/simulated_dispatcher.py
#   - FILE: core/utils/reflex_token_parser.py
#   - FILE: core/watchdog/timeout_recovery.py
#   - FILE: core/watchdog/watchdog_core.py
#   - FILE: core/watchdog/watchdog_manager.py

### Structure Changelog

| Date           |                                              Change Summary                                            |
|----------------|--------------------------------------------------------------------------------------------------------|
| **2025-04-20** | Structure Changelog resumes HERE in T03_SEQUENCING.txt".                                               |
| **2025-04-20** | Sequencer utility GUI launched from menu item "Launch Prompt Cycle Test".                              |
| **2025-04-22** | Files core/control/sequence_controller.py and core/boot/pathfix.py moved out of T02_INITIAL_PROCESSING |
| **2025-04-22** | Files core/control/sequence_controller.py and core/boot/pathfix.py moved into T03_SEQUENCING (THIS)    |
| **2025-04-23** | All issues with the sequencer have been fixed. It can now pass messages to the utility windows.        |
| **2025-04-23** | File core/utils/reflex_token_parser.py was created.                                                    |
|----------------|--------------------------------------------------------------------------------------------------------|

---



# === FILE NAME: core/control/sequence_controller.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/control/sequence_controller.py
Authors: ChatGPT and Mark
Created: 2025-04-16
Location: Evans, Colorado
Project: Aurora

Main orchestrator for executing sequence-driven reflex logic. This module reads
step instructions from the database, triggers reflex execution via dispatcher,
tracks jump/repeat state, and evaluates completion or interrupts.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T02_INITIAL_PROCESSING.txt
    - This file participates in the T02-B04_SEQ_CONT branch of development.
    - All session behaviors are tracked and logged through flat file modules.

---

Sequence Controller

Executes step-based sequences stored in the database. Supports step jumping,
bounded loops, reflex triggering, and future interrupt integration.
"""

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SEQUENCES BEGIN HERE. A DISPATCHER FUNCTION IS CALLED WHEN A DISPATCH MESSAGE HAS BEEN COMPOSED <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from data.db_interface import load_sequence_steps # *********************************** Chat with aurora.db to use the "load_sequence_steps" function <<<<<<<<<<<<<<<<<<<<<<<<

class SequenceController:
    def __init__(self, sequence_id: int, simulated: bool = False, simulator_gui=None):
        self.simulated = simulated
        self.simulator_gui = simulator_gui
        self.sequence_id = sequence_id
        self.steps = load_sequence_steps(sequence_id) # ******************************* Chat with aurora.db to retrieve the sequence id. This defines the steps to be retrieved...
        self.index = 0 # <<<<<<<<<<< Maybe 1-based instead?
        self.index_store = 0 # <<<<< Sequence control after?
        self.loop_count = 0 # <<<<<< Works afaik
        self.resume = 0 # <<<<<<<<<< Works afaik
        self.history = []
        self.halted = 0
        if simulated:
            from core.control import simulated_dispatcher as dispatcher # ************* If we're simulated, grab the "simulated_dispatcher" routine
        else:
            from core.control import reflex_dispatcher as dispatcher # **************** If we're not simulated, grab the "reflex_dispatcher" routine

        self.dispatch_step = dispatcher.dispatch_step # ******************************* CALL DISPATCH ROUTINE, RETURNS STEPS **********************

    def run(self): # ****************************************************************** Imaginatively named function "run" - run a loop

        print(f"[SequenceController] Starting sequence {self.sequence_id}...") # ****** Debug print sequence startup data
        # ***************************************************************************** RMEMEBER: STEPS ARE INDEXED FROM OUTSIDE THIS ROUTINE
        while self.index < len(self.steps):
            if self.resume:

                """ SUB STEP ROUTINE BLOCK
                if "substeps" in step and step.get("repeat_count", 0) < len(step["substeps"]):
                    substep_text = step["substeps"][step["repeat_count"]]
                    print(f"[SequenceController] Processing substep: {substep_text}")
                    result = self.dispatch_step({"instruction": substep_text})
                else:
                    result = self.dispatch_step(step)
                """

                step = self.steps[self.index_store] # ******************** While running after first pass - DB CALL <<<<
            else:
                step = self.steps[self.index] # ************************** For first entry - DB CALL <<<<

            print(f"[SequenceController] Length of self.steps is: {len(self.steps)}")
            print(f"[SequenceController] Executing step_order {step['step_order']} (index {self.index}): {step.get('instruction')}")
            from PySide6.QtCore import QMetaObject, Qt, Q_ARG

            prompt_text = (f"[SequenceController] Executing step_order {step['step_order']} (index {self.index}): {step.get('instruction')}") # <<<<<< UPDATE LOG WINDOW
            if self.simulator_gui:
                QMetaObject.invokeMethod(
                    self.simulator_gui,
                    #"update_injected_prompt", # ************************** Write to GUI "injected_prompt" (Top Box); this is defined in PromptSimulatorWindow
                    "update_step_log", # ********************************** Write to GUI "step_log" (Second from bottom); this is defined in PromptSimulatorWindow
                    #"update_response_log", # ***************************** Write to GUI "response_log" (Bottom box); this is defined in PromptSimulatorWindow
                    Qt.QueuedConnection,
                    Q_ARG(str, prompt_text)
                )

            result = self.dispatch_step(step) # <<<<<<<<<<<<<<<<<<<<<<<<<<< CALL THE DISPATCHER

            if isinstance(result, str) and "step complete" in result: # ************* If the function returned "step complete" <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
                print(f"[SequenceController] Step {self.index} Complete. Awaiting next trigger. Holding...")
                self.index += 1
                self.resume = 1
                self.index_store = self.index

                prompt_text = (f"[SequenceController] Step {self.index} Complete. Awaiting next trigger. Holding...") # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< UPDATE LOG WINDOW
                from PySide6.QtCore import QMetaObject, Qt, Q_ARG

                if self.simulator_gui:
                    QMetaObject.invokeMethod(
                        self.simulator_gui,
                        #"update_injected_prompt", # ********************** Write to GUI "injected_prompt" (Top Box); this is defined in PromptSimulatorWindow
                        #"update_step_log", # ***************************** Write to GUI "step_log" (Second from bottom); this is defined in PromptSimulatorWindow
                        "update_response_log", # ************************** Write to GUI "response_log" (Bottom box); this is defined in PromptSimulatorWindow
                        Qt.QueuedConnection,
                        Q_ARG(str, prompt_text)
                    )
                continue

            elif isinstance(result, str) and "step incomplete" in result: # ************ If the function returned "step incomplete" **************
                print(f"[SequenceController] Step {self.index} incomplete due to missing response trigger. Holding...")
                self.resume = 1
                self.index_store = self.index

                prompt_text = (f"[SequenceController] Step {self.index} Inomplete. Awaiting retry. Holding...") # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< UPDATE LOG WINDOW
                from PySide6.QtCore import QMetaObject, Qt, Q_ARG

                if self.simulator_gui:
                    QMetaObject.invokeMethod(
                        self.simulator_gui,
                        #"update_injected_prompt", # ********************** Write to GUI "injected_prompt" (Top Box); this is defined in PromptSimulatorWindow
                        #"update_step_log", # ***************************** Write to GUI "step_log" (Second from bottom); this is defined in PromptSimulatorWindow
                        "update_response_log", # ************************** Write to GUI "response_log" (Bottom box); this is defined in PromptSimulatorWindow
                        Qt.QueuedConnection,
                        Q_ARG(str, prompt_text)
                    )
                continue

            elif isinstance(result, str) and "HALT" in result: # ********** IF THE FUNCTION RETURNED "HALT"
                self.index = 6 # Sets self.index to "6" for this experiment; NEEDS TO EQUAL STEP COUNT
                self.halted = 1
            else:
                print(f"[SequenceController] Step {self.index} would have runaway. Result was: {result}") # ###### CATCH RUNAWAY
                self.resume = 0
                continue

            self.history.append({
                "index": self.index,
                "step": step,
                "result": result
            })

            # Handle per-step repeat logic
            step_repeat = step.get("repeat", 0)
            if step_repeat > 0:
                if "repeat_count" not in step:
                    step["repeat_count"] = 1
                else:
                    step["repeat_count"] += 1

                if step["repeat_count"] < step_repeat:
                    print(f"[SequenceController] Repeating step {self.index} ({step['repeat_count']}/{step_repeat})")
                    continue
                else:
                    print(f"[SequenceController] Step {self.index} reached repeat limit. Continuing.")
                    step["repeat_count"] = 0  # Optional cleanup

            # ############################################################################################################## CHECK FOR JUMP
            if step.get('jump') not in (None, 0):
                target_step_order = step['jump']
                rpt = step.get('jmp_rpt', 0)

                if rpt == 0 or self.loop_count < rpt:
                    # Locate the step with matching step_order
                    target_index = next(
                        (i for i, s in enumerate(self.steps) if s["step_order"] == target_step_order),
                        None
                    )

                    if target_index is None:
                        print(f"[SequenceController] ERROR: jump target step_order {target_step_order} not found. Skipping jump.")
                    else:
                        loop_display = f"{self.loop_count + 1}/{rpt}" if rpt else f"{self.loop_count + 1}/(infinite)"

                        if target_index == self.index and rpt == 0:
                            print(f"[SequenceController] ERROR: Infinite self-jump detected at index {self.index}. Aborting sequence.")
                            break

                        print(f"[SequenceController] Jumping to step_order {target_step_order} (index {target_index}) â€” loop {loop_display}")
                        self.index = target_index
                        self.loop_count += 1
                        continue
                else:
                    print("[SequenceController] Jump limit reached. Continuing.")
                    self.loop_count = 0

        self.index += 1 # #################################################################################################### IF EVERYTHING PASSES, INCREMENT SELF.INDEX

        # #################################################################################################################### END LOOP
        if self.halted == 0:
            print(f"[SequenceController] Sequence {self.sequence_id} completed.")
            prompt_text = (f"[SequenceController] Sequence {self.sequence_id} completed.") # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< UPDATE LOG WINDOW
            from PySide6.QtCore import QMetaObject, Qt, Q_ARG

            if self.simulator_gui:
                QMetaObject.invokeMethod(
                    self.simulator_gui,
                    #"update_injected_prompt",  # ******************** Write to GUI "injected_prompt" (Top Box); this is defined in PromptSimulatorWindow
                    "update_step_log", # ***************************** Write to GUI "step_log" (Second from bottom); this is defined in PromptSimulatorWindow
                    #"update_response_log", # ************************ Write to GUI "response_log" (Bottom box); this is defined in PromptSimulatorWindow
                    Qt.QueuedConnection,
                    Q_ARG(str, prompt_text)
                )

        if self.halted == 0 and self.simulator_gui and hasattr(self.simulator_gui, 'on_sequence_complete'):
            self.simulator_gui.on_sequence_complete()
        elif self.halted == 1 and self.simulator_gui and hasattr(self.simulator_gui, 'on_sequence_halted'):
            self.simulator_gui.on_sequence_halted()

if __name__ == "__main__":
    from data.db_interface import get_sequence_id_by_name

    seq_id = get_sequence_id_by_name("SHOW MODE")
    if seq_id is None:
        print("[SequenceController] ERROR: Sequence 'SHOW MODE' not found.")
        sys.exit(1)

    controller = SequenceController(sequence_id=seq_id)
    controller.run()



# === FILE NAME: core/boot/pathfix.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/boot/pathfix.py
Authors: ChatGPT and Mark
Created: 2025-04-17
Location: Evans, Colorado
Project: Aurora

This utility module provides a centralized method to patch the Python sys.path
during local development. It ensures consistent import resolution from any
submodule within the Aurora project when executed in non-package mode.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T02_INITIAL_PROCESSING.txt
    - Category: Bootstrap Utility
    - Used by all modules requiring guaranteed path stability across contexts.

---

Path Bootstrap Utility

Call patch_path() at the top of any deep module to ensure that the root
project path is visible to Python, enabling absolute imports of all
core.* modules and subpackages.
"""

import sys
import os

def patch_path():
    root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if root not in sys.path:
        sys.path.insert(0, root)
        print(f"[PATHFIX] Root path added: {root}")
    else:
        print("[PATHFIX] Root path already present.")



# === FILE NAME: core/gui/prompt_simulator_window.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/gui/prompt_simulator_window.py
Authors: ChatGPT and Mark
Created: 2025-04-20
Location: Evans, Colorado
Project: Aurora

This module implements the Prompt Cycle Simulation Utility. It replaces live browser
interaction with a GUI mock environment for testing step-based prompt sequences without
launching ChatGPT or consuming tokens. Prompts intended for the browser are intercepted,
and a manual response may be entered to simulate GPT output.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T03_SEQUENCING.txt
    - This file participates in the T02-B04_SEQ_CONT branch of development.
    - All session behaviors are tracked and logged through flat file modules.

---

Prompt Simulation Utility

Displays prompts that would be issued by the reflex dispatcher,
allows user to supply simulated responses, and provides UI feedback
for step flow and interaction without launching a browser.
"""

from PySide6.QtWidgets import (
    QDialog, QVBoxLayout, QHBoxLayout,
    QLabel, QListWidget, QTextEdit, QLineEdit, QPushButton
)

from PySide6.QtCore import QThread, QObject, Signal, Slot, Qt, QMetaObject

from core.control import simulated_dispatcher

class PromptSimulatorWindow(QDialog): # ####################################### DEFINE GUI
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Aurora â€“ Prompt Cycle Simulator")
        self.setMinimumSize(700, 800) # Width, Height

        self.layout = QVBoxLayout(self)

        # --- Begin Sequence Button ---
        self.run_button = QPushButton("Run Sequence") # ************************ "Run sequence" button
        self.run_button.clicked.connect(self.begin_sequence)
        self.layout.addWidget(self.run_button)

        # --- Prompt Preview ---
        self.prompt_label = QLabel("Injected Prompt (Simulated):") # *********** Injected Prompt (Simulated) listbox
        self.prompt_display = QTextEdit()
        self.prompt_display.setReadOnly(True)

        # --- Simulated GPT Response ---
        self.reply_label = QLabel("Enter Simulated GPT Response:") # *********** User entry box
        self.reply_input = QLineEdit()
        self.send_button = QPushButton("Send Response") # ********************** "Send Response" button
        self.send_button.clicked.connect(self.send_response)

        # --- Step History and Response Log ---
        self.step_log = QListWidget()
        self.step_log.setFixedHeight(300)
        self.step_log_label = QLabel("Sequence Step Log:") # ******************** Sequencer Step Log listbox

        self.reply_log = QListWidget()
        self.reply_log.setFixedHeight(200)
        self.reply_log_label = QLabel("Simulated Response Log:") # ************** Sequencer Response Log listbox

        self.status_label = QLabel("Status: Idle") # **************************** Sequencer status label
        self.layout.addWidget(self.status_label)

        # --- Layout Assembly ---
        self.layout.addWidget(self.prompt_label) # ****************************** Injected Prompt label
        self.layout.addWidget(self.prompt_display) # **************************** Injected Prompt text edit box
        self.layout.addWidget(self.reply_label) # ******************************* User entry box label

        hbox = QHBoxLayout()
        hbox.addWidget(self.reply_input) # ************************************** User entry box
        hbox.addWidget(self.send_button) # ************************************** User entry box send button
        self.layout.addLayout(hbox) # ******************************************* Horizonal container

        self.layout.addWidget(self.step_log_label) # **************************** Step Log label
        self.layout.addWidget(self.step_log) # ********************************** Step Log listbox

        self.layout.addWidget(self.reply_log_label) # *************************** Response Log box label
        self.layout.addWidget(self.reply_log) # ********************************* Respons Log listbox

        self.sequence_running = False

        from core.control.simulated_dispatcher import inject_simulator # ******** Call function inject_simulator
        inject_simulator(self)

    @Slot(str)
    def update_injected_prompt(self, prompt_text: str): # *********************** DISPLAYS SIMULATED GPT RESPONSE BLOCKS IN "prompt_display"
        self.prompt_display.setPlainText(prompt_text)
        #self.step_log.addItem(f"[Injected] {prompt_text[:80]}")
        print(f"[Prompt Injected Display Updated] {prompt_text[:80]}")

    @Slot(str)
    def update_step_log(self, prompt_text: str): # ****************************** DISPLAYS STEPS IN "step_log"
        self.step_log.addItem(f"[Injected] {prompt_text[:80]}")
        print(f"[PromptSimulatorWindow Injected] {prompt_text[:80]}")

    @Slot(str)
    def update_response_log(self, prompt_text: str): # ************************** DISPLAYS USER RESPONSES IN "reply_log"
        self.reply_log.addItem(f"[Injected] {prompt_text[:80]}")
        print(f"[PromptSimulatorWindow Injected] {prompt_text[:80]}")

    def send_response(self): # ************************************************** RECEIVES USER RESPONSE FROM "reply_input"
        response = self.reply_input.text().strip()
        if response:
            #self.reply_log.addItem(f"[User] {response}") # ##################### REPLY LOG
            print(f"[PromptSimulatorWindow Added User Response:] {response}")
            self.reply_input.clear()
            #self.prompt_display.clear()
            simulated_dispatcher.response_queue.put(response) # ################# DISPATCHER RESPONSE_QUEUE CRITICAL - USED IN DISPATCHERS

    def begin_sequence(self):
        if not self.sequence_running:
            self.sequence_running = True # ########################################## START THE SEQUENCE IF NOT RUNNING
            self.status_label.setText("Status: Running...") # ####################### SET STATUS LABEL TEXT
            self.run_button.setText("Halt Sequence") # ############################## SET RUN BUTTON TEXT
            try:
                seq_id = self.parent().ui.pb_sequence_arm.property("sequence_id") # # Grab sequence id from the arm button, assign to "seq_id"
            except AttributeError:
                print("[PromptSimulatorWindow] Could not access sequence ID.") # #### If oops
                return

            if seq_id is None:
                print("[PromptSimulatorWindow] No sequence ID armed.") # ############ If oops
                return

            self.thread = QThread()
            self.runner = SequenceRunner(seq_id, self)
            self.runner.moveToThread(self.thread)
            self.thread.started.connect(self.runner.run)
            self.runner.finished.connect(self.thread.quit)
            self.runner.finished.connect(self.runner.deleteLater)
            self.thread.finished.connect(self.thread.deleteLater)

            print(f"[PromptSimulatorWindow] Running sequence {seq_id} in background thread.")
            self.thread.start() # *************************************************** LAUNCH SEQUENCER ROUTINES
        else:
            self.sequence_running = False
            self.status_label.setText("Status: Halted") # ########################### SET STATUS LABEL TEXT
            self.run_button.setText("Restart Sequence") # ########################### SET RUN BUTTON TEXT
            response = "HALT"
            print(f"[PromptSimulatorWindow Halt Command Received:] {response}")
            simulated_dispatcher.response_queue.put(response) # ##################### DISPATCHER RESPONSE_QUEUE CRITICAL - USED IN DISPATCHERS

    @Slot()
    def on_sequence_complete(self):
        self.status_label.setText("Status: Sequence complete.")

    @Slot()
    def on_sequence_halted(self):
        self.status_label.setText("Status: Sequence halted.")
        self.reply_input.clear() # ################################################## CLEAR REPLY INPUT
        self.step_log.clear() # ##################################################### CLEAR STEP LOG
        self.reply_log.clear() # #################################################### CLEAR REPLY LOG

class SequenceRunner(QObject): # #################################################### DEFINE THE SEQUENCE RUNNER
    finished = Signal()

    def __init__(self, sequence_id, simulator_gui):
        super().__init__()
        self.sequence_id = sequence_id
        self.simulator_gui = simulator_gui

    @Slot()
    def run(self):
        import threading
        print(f"[DEBUG] Running in thread: {threading.current_thread().name}")
        from core.control.sequence_controller import SequenceController
        controller = SequenceController(sequence_id=self.sequence_id, simulated=True, simulator_gui=self.simulator_gui)# ************ Assign values to pass to sequence controller
        controller.run() # ********************************************************** RUN SEQUENCE CONTROLLER
        QMetaObject.invokeMethod(
            self.parent(), # ######################################################## Assumes parent is PromptSimulatorWindow
            "on_sequence_complete",
            Qt.QueuedConnection
        )
        self.finished.emit()
        print(f"[DEBUG] THREAD COMPLETE: {threading.current_thread().name}")

if __name__ == "__main__":
    from PySide6.QtWidgets import QApplication
    import sys
    app = QApplication(sys.argv)
    window = PromptSimulatorWindow()
    window.show()
    simulated_dispatcher.inject_simulator(window)
    sys.exit(app.exec())



# === FILE NAME: core/control/simulated_dispatcher.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/control/simulated_dispatcher.py
Authors: ChatGPT and Mark
Created: 2025-04-20
Location: Evans, Colorado
Project: Aurora

Simulated dispatcher module that replaces browser interaction with GUI-driven
manual prompt/response exchange. Used for development, testing, and verification
of sequence behavior without launching an actual ChatGPT session.

04/23/2025 - UNDER EXAMINATION FOR REFACTOR - REFLEX_TOKEN_PARSER.PY WILL PASS STEP PERMISSIVES

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T03_SEQUENCING.txt
    - This file participates in the T02-B04_SEQ_CONT branch of development.
    - Used in place of reflex_dispatcher during simulation mode.
---

Simulated Reflex Dispatcher

Provides prompt injection and wait emulation without launching a browser.
Prompts appear in the PromptSimulatorWindow; replies are entered manually.
"""

import time
import os
import sys
from queue import Queue
from core.utils.reflex_token_parser import parse_reflex_tokens_with_args
from core.api import launch_app, open_folder  # From routed __init__.py (platform-safe)

# Ensure 'data' is in sys.path for correct resolution of db_interface
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from data.db_interface import resolve_reflex_action

# This global is populated externally
simulator_instance = None
response_queue = Queue() # ########################## DISPATCHER RESPONSE_QUEUE DEFINED IN PROMPT_SIMULATOR_WINDOW - CRITICAL ##############


def inject_simulator(sim): # ************************************* Called by prompt_simulator_window
    global simulator_instance
    simulator_instance = sim
    print("[SimulatedDispatcher] Simulator connected.")

def dispatch_step(step: dict) -> str: # *************************** Define function "dispatch_step"
    """
    Handle a single sequence step in simulation mode.
    Instead of browser actions, prompt and response are handled via UI.
    """
    if simulator_instance is None:
        print("[SimulatedDispatcher] ERROR: No simulator attached.")
        return "(Simulator not ready)"

    expected_id  = step.get("expected", 0) # ********************** Get the value from the "expected" field and assign it to expected_id
    if expected_id: # ********************************************* If the value in expected_id is not empty
        try:
            expected = resolve_reflex_action(expected_id) # ******* Chat with aurora.db to return the value in the "expected_id" field and assign it to "expected"
            if expected: # **************************************** If expected is not empty
                print(f"[SimulatedDispatcher] Resolved expected {expected_id} to expected: {expected}") # **** Debug print resolved value in expected
            else:
                print(f"[SimulatedDispatcher] ERROR: expected {expected_id} returned nothing.") # ************ Debug print failure to resolve value in expected_id
        except Exception as e:
            print(f"[SimulatedDispatcher] ERROR resolving expectd {expected_id}: {e}") # ********************* Debug print error resolving value in expected_id

    # *************************************************************************** If no command, attempt reflex resolution

    reflex_id = step.get("reflex_action", 0) # <- <- <- <- <- <- CAUSE OF THREAD RUNAWAY <- <- <- <- <- <-
    if reflex_id:
        try:
            reflex = resolve_reflex_action(reflex_id)
            if reflex:
                print(f"[SimulatedDispatcher] Resolved reflex_action {reflex_id} to reflex: {reflex}")
            else:
                print(f"[SimulatedDispatcher] ERROR: reflex_action {reflex_id} returned nothing.")
        except Exception as e:
            print(f"[SimulatedDispatcher] ERROR resolving reflex_action {reflex_id}: {e}")

    # *************************************************************************** Still nothing? Bail. <- <- <- <- <- <- WON'T HAPPEN AFTER REFACTOR
    if not expected:
        print("[SimulatedDispatcher] ERROR: Step has no value in the field named  expected. Skipping.")
        return "(Value for expected not found in step)"

    elif not reflex:
        print("[SimulatedDispatcher] ERROR: Step has no value in the field named reflex. Skipping.")
        return "(Value for expected not found in step)"

    # *************************************************************************** Check the contents of the "expected" field <- <- <- <- <- <- AUTO TO NEXT LINE? FIRST EXECUTION?
    if expected: # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> We have a value in "expected" <<<<<<<<<<<<<<<<<<
        print(f"[SimulatedDispatcher] Command: {expected_id}; command: {expected}")
        # Block until simulated response is received
        print("[SimulatedDispatcher] Waiting for prompt input...")
        while response_queue.empty(): # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> RESPONSE QUEUE HOLD HERE <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
            time.sleep(0.1)

        reply = response_queue.get() # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> We have the simulated response representing that expected from the AI <<<<<<<<<<<<<<<<<<<<
        print(f"[SimulatedDispatcher] Received simulated reply: {reply}")
        if reply == "HALT":
            return reply

        # ==========================================================
        # Parse reflex tokens and attempt API command dispatch
        # ==========================================================
        #from core.utils.reflex_token_parser import parse_reflex_tokens_with_args
        #from core.api import launch_app, open_folder  # Routed by OS

        """
        Parses reflex-style tokens of the format /TOKEN/ or /TOKEN: ARGUMENT/ from response text.

        Args:
            response_text (str): The full GPT response or any string Aurora should parse.
            expected (str, optional): If provided, the parser will check if the final token matches this.

        Returns:
            dict: {
                "expected_matched": bool,  # Whether the final token matched expected.
                "finalizer": str or None,  # The last token found.
                "all_tokens": list of str, # All /TOKEN/ strings found.
                "arguments": dict,         # Mapping of each token to its ARGUMENT (or None if not present).
                "raw": str,                # The original response text.
                "match_reason": str,       # Explanation if no tokens were found.
            }
        """

        parsed = parse_reflex_tokens_with_args(reply, expected) # >>>>>>>>>>>>>>>>>>>>>>>> Supply the parsing function with "reply" and "expected" for comparison.

        if parsed["expected_matched"]: # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> If the function returns this string, "expected_matched".
            print("[SimulatedDispatcher] Expected token matched. Proceeding with reflex command(s)...")

            for token, arg in parsed["arguments"].items(): # >>>>>>>>>>>>>>>>>>>>>>>>>>>>> Test for specific reflex tokens, in this instance "system commands"....
                if token == "/LAUNCH APP/": # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> In this instance looking for the string "/LAUNCH APP/"...
                    result = launch_app(arg)# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> If so launch this...
                    print(f"[SimulatedDispatcher] [EXEC] launch_app('{arg}') â†’ {result}")
                elif token == "/OPEN FOLDER/": # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> In this instance looking for the string "/OPEN FOLDER/"...
                    result = open_folder(arg)# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> If so launch this...
                    print(f"[SimulatedDispatcher] [EXEC] open_folder('{arg}') â†’ {result}")

            return "(Simulated) Trigger match: step complete."
        else:
            print(f"[SimulatedDispatcher] Finalizer '{expected}' not found in reply. Holding at step.")
            return "(Simulated) Trigger missing: step incomplete."


        #  *********************************************************************** Check for trigger in the "expected" field <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        expected_key_id = step.get("reflex_action", 0)# ************************** Assign the string value in the "reflex_action" field
        expected_token = None

        if expected_key_id:# ***************************************************** If the "expected_key_id" value is not empty
            try:
                expected_token = resolve_reflex_action(expected_key_id)# ********* Chat with aurora.db to see if there is a match to expected_token. <- <- <- <- <- <- MATCH TOKEN
            except Exception as e:
                print(f"[SimulatedDispatcher] WARNING: Could not resolve expected token {expected_key_id}: {e}")# *************** If oops

            if expected_token:# ************************************************** If the "expected_token" value is not empty
                if expected_token in reply:# ************************************* If expected_token matches the user input
                    print(f"[SimulatedDispatcher] Expected token '{expected_token}' FOUND in reply.")#*************************** Debug print token found
                    return "(Simulated) Trigger match: step complete."# ********** Back to routine report "step complete" >>>>>>>>>>>>>>>>>
                else:
                    print(f"[SimulatedDispatcher] Expected token '{expected_token}' NOT FOUND. Holding at this step.")# ********* Debug print token not found
                    return "(Simulated) Trigger missing: step incomplete."# ****** Back to routine report "step incomplete" >>>>>>>>>>>>>>>
        else:
            print("[SimulatedDispatcher] No expected token defined. Proceeding by default.")# ******** If the "expected_key_id" value is empty
            return reply # ******************************************************** Back to routine return user input

    else:
        print(f"[SimulatedDispatcher] Unhandled command: {expected}")
        return f"(Simulated) Unhandled: {expected}"

    print(f"[SimulatedDispatcher] Prompt sent to simulator: {reply}")



# === FILE NAME: core/utils/reflex_token_parser.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/utils/reflex_token_parser.py
Authors: ChatGPT and Mark
Created: 2025-04-23
Location: Evans, Colorado
Project: Aurora

This module implements a parser for reflex-style tokens that adhere to the format
/TOKEN/ or /TOKEN: ARGUMENT/ as used by Aurora's reflex protocol. It extracts any tokens
found within a GPT response and returns a structured dictionary including the final
token (which acts as a step finalizer), all tokens encountered, any accompanying arguments,
and the original response text. This functionality is essential for determining if a response
meets the expected completion criteria and for capturing metadata for downstream processing.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of the
    Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: TEMPLATES.txt
    - Conforms to Auroraâ€™s reflex token protocol design.
"""

import re

def parse_reflex_tokens_with_args(response_text, expected=None):
    """
    Parses reflex-style tokens of the format /TOKEN/ or /TOKEN: ARGUMENT/ from response text.

    Args:
        response_text (str): The full GPT response or any string Aurora should parse.
        expected (str, optional): If provided, the parser will check if the final token matches this.

    Returns:
        dict: {
            "matched": bool,           # Whether the final token matched expected.
            "finalizer": str or None,  # The last token found.
            "all_tokens": list of str, # All /TOKEN/ strings found.
            "arguments": dict,         # Mapping of each token to its ARGUMENT (or None if not present).
            "raw": str,                # The original response text.
            "reason": str,             # Explanation if no tokens were found.
        }
    """
    pattern = r"/([A-Z_]+)(?::([^/]+))?/"
    matches = re.findall(pattern, response_text)

    if not matches:
        return {
            "matched": False,
            "finalizer": None,
            "all_tokens": [],
            "arguments": {},
            "raw": response_text,
            "reason": "No tokens found"
        }

    tokens = [f"/{token}/" for token, _ in matches]
    final_token = tokens[-1]
    matched = final_token == expected if expected else True

    return {
        "matched": matched,
        "finalizer": final_token,
        "all_tokens": tokens,
        "arguments": {f"/{token}/": arg.strip() if arg else None for token, arg in matches},
        "raw": response_text
    }



# === FILE NAME: core/watchdog/timeout_recovery.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/watchdog/timeout_recovery.py
Authors: ChatGPT and Mark
Created: 2025-04-25
Location: Evans, Colorado
Project: Aurora

This module defines structured recovery routines invoked when
the watchdog timer triggers due to session timeout, step failure,
or missing reflex completion. Recovery strategies include retries,
sequence aborts, or diagnostic escalation based on reflex context.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T03_SEQUENCING.txt
    - Category: Reflex Recovery / Watchdog Systems
    - Interfaces with: watchdog_core.py, sequence_controller.py
"""

def retry_current_step(context):
    """
    Attempt to retry the current step after timeout.
    Context may include sequence ID, current index, and retry policy.
    """
    print("[TimeoutRecovery] Retrying current step... (Placeholder)")
    # TODO: Implement step retry logic
    return "(TimeoutRecovery) Retry triggered."

def abort_sequence(context):
    """
    Abort the running sequence after unrecoverable timeout.
    """
    print("[TimeoutRecovery] Aborting sequence... (Placeholder)")
    # TODO: Implement sequence abort and session cleanup
    return "(TimeoutRecovery) Sequence aborted."

def diagnose_timeout(context):
    """
    Generate a diagnostic report after timeout event.
    """
    print("[TimeoutRecovery] Generating timeout diagnostic report... (Placeholder)")
    # TODO: Capture session state, timing history, and active reflex info
    return "(TimeoutRecovery) Diagnostic report generated."



# === FILE NAME: core/watchdog/watchdog_core.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/watchdog/watchdog_core.py
Authors: ChatGPT and Mark
Created: 2025-04-25
Location: Evans, Colorado
Project: Aurora

This module implements the core watchdog timer that monitors elapsed time
following a reflex action dispatch. If a finalizer token or expected response
is not received within the configured timeout window, a recovery action
is triggered. This prevents Aurora from becoming stalled in non-responsive
or partial states.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T03_SEQUENCING.txt
    - Category: Reflex Safety / Watchdog Systems
    - Interfaces with: sequence_controller.py, simulated_dispatcher.py, timeout_recovery.py
"""

import threading
import time

class WatchdogTimer:
    def __init__(self, timeout_seconds, on_timeout_callback):
        self.timeout_seconds = timeout_seconds
        self.on_timeout_callback = on_timeout_callback
        self.timer_thread = None
        self.start_time = None
        self.running = False

    def start(self):
        if self.running:
            self.cancel()
        self.running = True
        self.start_time = time.time()
        self.timer_thread = threading.Thread(target=self._watch)
        self.timer_thread.daemon = True
        self.timer_thread.start()

    def _watch(self):
        while self.running:
            if time.time() - self.start_time >= self.timeout_seconds:
                self.running = False
                if self.on_timeout_callback:
                    self.on_timeout_callback()
            time.sleep(0.1)

    def cancel(self):
        self.running = False



# === FILE NAME: core/watchdog/watchdog_manager.py ===

"""
Aurora â€“ Reflexive AI Control Framework
---------------------------------------

Module: core/watchdog/watchdog_manager.py
Authors: ChatGPT and Mark
Created: 2025-04-25
Location: Evans, Colorado
Project: Aurora

This module provides high-level control over the watchdog system,
handling timer initialization, cancellation, and timeout escalation
via recovery routines. It acts as the central interface between
runtime step execution and reflexive fault handling.

License:
    This file is part of the Aurora project and is distributed under the terms of
    the MIT License. See the LICENSE file in the project root for details.

WARNING:
    This file may be auto-modified by development tools or AI agents as part of
    the Aurora project workflow. Manual changes should be made cautiously.
    (Meddle if you dare, foolish mortal!)

FLAT Compliance:
    - Registered in: T03_SEQUENCING.txt
    - Category: Reflex Supervision / Watchdog Manager
    - Interfaces with: watchdog_core.py, timeout_recovery.py, sequence_controller.py
"""

from core.watchdog.watchdog_core import WatchdogTimer
from core.watchdog import timeout_recovery

# Global watchdog timer instance
_active_watchdog = None

def start_watchdog(timeout_seconds, context=None):
    """
    Start a new watchdog timer for the current step or reflex operation.
    """
    global _active_watchdog
    print(f"[WatchdogManager] Starting watchdog timer: {timeout_seconds} seconds")

    def on_timeout():
        print("[WatchdogManager] Watchdog triggered timeout event.")
        timeout_recovery.retry_current_step(context)

    _active_watchdog = WatchdogTimer(timeout_seconds, on_timeout)
    _active_watchdog.start()

def cancel_watchdog():
    """
    Cancel any active watchdog timer.
    """
    global _active_watchdog
    if _active_watchdog:
        print("[WatchdogManager] Cancelling active watchdog timer.")
        _active_watchdog.cancel()
        _active_watchdog = None




[END T03_SEQUENCING.txt]
